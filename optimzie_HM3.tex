\documentclass{ctexart}
\usepackage[]{amsmath}
\usepackage[]{graphicx}
\usepackage[]{algorithm}
\usepackage[]{algorithmicx}
\usepackage[]{algpseudocode} 
\begin{document}
\title{最优化第三次上机作业}
\author{王恒亮 \quad 学号：1601210102}
\date{}
\maketitle
本次实现了解决Minimax Problems的三种方法，分别为Least pth Method\cite{Charalambous1979}, Smoothing Method\cite{Xu2001}和Adaptive Smoothing Method\cite{Polak2003}。以下分别介绍算法及程序实现，实验结果及分析。
\section{算法及程序实现}
\subsection{Least pth Method}
Least pth Method 主要通过优化U函数来解决Minimax问题，U函数的定义如下：
\begin{equation}
	\label{eq:ufun}
	U(x,u,p,\xi)=\left\{
		\begin{align}
			M(x,\xi)(\sum_{i\in S(x,\xi)}{u_i(\frac{f_i(x)-\xi}{M(x,\xi)})^q})^{1/q}, for M(x,\xi) \neq 0,\\
		0, for M(x,\xi) = 0
		\end{align}\right.
\end{equation}
其中:
\begin{align}
	u_i \geq 0 \text{and} \sum_{i\in I} = 1\\
	M(x, \xi) = \max_{i\in I}{(f_i(x) - \xi)}
\end{align}
算法的框架如下：
\begin{description}
\item 取$p>1, \xi^{(1)} =0,u_i^{(1)}=1,i\in I,x=x^0,r=1$
\item 固定$u,p,\xi$，在公式\ref{eq:ufun}中对x取极小，最优解设为$x^r$
\item 更新$u$
	\[u^{r+1}_i\frac{v_i^{r+1}}{\sum_{j\in I}{v_j^{r+1}}}, i\in I\]
	其中：
	\[v_{i}^{r+1}=\left\{
		\begin{align}
			u_i^r(\frac{f_i(x^r)-\xi}{M(x^r,\xi)})^{q-1}, for i \in S(x^r,\xi)\\
			0 for i\in I - S(x^r, \xi)
		\end{align}\right.\]
	\[S(x^r, \xi)=\left\{
		\begin{align}
			\{i|f_i(x)-\xi>0,i\in I\}, if M(x^r,\xi) \geq 0\\
			I, if M(x^r,\xi) <0
		\end{align}\right.\]
\item 更新$\xi$
	\[\xi^{r+1} = \left\{
		\begin{align}
			\xi^r + M(x^r, \xi^r) , if M(x^r, \xi) < 0\\
			\xi^r + \lambda M(x^r, \xi^r)
		\end{align}\right.\]
\item 更新$p, p = cp, c\geq 1$，转至第2步
\end{description}
在实现中，除了计算出了最优点和最优点的函数值之外，还记录了迭代次数，函数调用次数，具体的参数设置将在结果中说明。
\subsection{Smoothing Method}
Smoothing Method是将Minimax问题转化为优化近似函数$f(x,\mu)$，其定义如下：
\[f(x,\mu) = \mu\ln\sum_{i=1}^{m}{exp(\frac{f_i(x)}{\mu})}\]
对于$f(x,\mu)$的优化使用牛顿法求出下降方向，使用Armijo准则找到步长，在每步迭代后会更新$\mu, \mu = \beta\mu$。对于$f(x,\mu)$的函数，梯度和Hessian矩阵的计算如下：
$$
\begin{aligned}
f(x^k,\mu_k)=f(x^k)+\mu_k\ln\sum_{i=1}^{m}{exp(\frac{f_i(x^k)-f(x^k)}{\mu_k})}\\
\triangledown_xf(x,\mu)=\sum_{i=1}^m{\lambda_i(x,\mu)\triangledown f_i(x)}\\
\triangledown^2_xf(x,\mu)=&\sum_{i=1}^m(\lambda_i(x,\mu)\triangledown^2f_i(x)+\frac{1}{\mu}\lambda_i(x,\mu)\triangledown f_i(x)\triangledown f_i(x)^T)\\
			& -\frac{1}{mu}(\sum_{i=1}^{m}\lambda_i(x,\mu)\triangledown f_i(x))(\sum_{i=1}^m{\lambda_i(x,\mu)\triangledown f_i(x)})^T\\
\lambda_i(x^k,\mu_k) = \frac{exp(\frac{f_i(x^k)-f(x^k)}{\mu_k})}{\sum_{j=1}^{m}{exp(\frac{f_j(x^k)-f(x^k)}{\mu_k})}}
\end{aligned}
$$
\subsection{Adaptive Smoothing Method}
Adaptive Smoothing Method是在Smoothing Method的基础对算法做了改进，主要解决了在$\mu\rightarrow 0$时产生的ill-condition问题。具体算法如下：
\begin{description}
\item 取$i=0,k=0,\gamma = 1$
\item 设$\Phi_{p_i,xx}(x_i)$是Hessian矩阵R的Cholesky修正，$c(R)$为R的条件数的倒数，则若$\Phi_{p_i,xx}(x_i)$正定且$c(R)\geq k_1,p_i\leq k_3$，转步3，若$\Phi_{p_i,xx}(x_i)$正定且$c(R)\geq k_1$且$\Phi_{p_i,xx}(x_i)$的最大特征值$\sigma_{p_i,max}(x_i)$满足$\sigma_{p_i,max}\leq k_2$，转步3，否则转步4。
\item $h_{p_i}(x_i) = -\Phi_{p_i,xx}(x_i)^{-1}\triangledown\Phi_{p_i}(x_i)$，转步5
\item $h_{p_i}(x_i) = -\triangledown_{p_i}(x_i)$
\item 计算步长
	\[\lambda_{p_i}(x_i) = \max_{l\in N}\{\beta^l|\Phi_{p_i}(x_i+\beta^lh_{p_i}(x_i))-\Phi_{p_i}(x_i)\leq \alpha\beta^l<\triangledown\Phi_{p_i}(x_i),h_{p_i}(x_i)>\}\]
\item $x_{i+1} = x_i + \lambda_{p_i}(x_i)h_{p_i}(x_i)$
\item 
\end{description}
